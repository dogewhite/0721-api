#!/usr/bin/env python3
"""
LLMç®¡ç†å™¨æµ‹è¯•è„šæœ¬
"""

import sys
import os
import asyncio
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm_manager import llm_manager

def test_llm_manager():
    """æµ‹è¯•LLMç®¡ç†å™¨"""
    try:
        print("ğŸš€ å¼€å§‹æµ‹è¯•LLMç®¡ç†å™¨...")
        
        # 1. æµ‹è¯•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        print("\n1. è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        models = llm_manager.get_available_models()
        for model in models:
            print(f"   - {model['name']} ({model['id']}) - {model['description']}")
        
        if not models:
            print("   âš ï¸  æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
            return
        
        # 2. æµ‹è¯•åŒæ­¥èŠå¤©
        print("\n2. æµ‹è¯•åŒæ­¥èŠå¤©:")
        for model in models[:2]:  # åªæµ‹è¯•å‰2ä¸ªæ¨¡å‹
            try:
                print(f"   æ­£åœ¨æµ‹è¯• {model['name']}...")
                response = llm_manager.simple_chat(
                    "è¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±",
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚",
                    model['id']
                )
                print(f"   {model['name']} å›å¤: {response[:100]}...")
            except Exception as e:
                print(f"   âŒ {model['name']} æµ‹è¯•å¤±è´¥: {e}")
        
        print("\nâœ… åŒæ­¥èŠå¤©æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ LLMç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_llm_manager_async():
    """æµ‹è¯•LLMç®¡ç†å™¨å¼‚æ­¥åŠŸèƒ½"""
    try:
        print("\n3. æµ‹è¯•å¼‚æ­¥èŠå¤©:")
        
        models = llm_manager.get_available_models()
        if not models:
            print("   âš ï¸  æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return
        
        # æµ‹è¯•å¼‚æ­¥èŠå¤©
        for model in models[:2]:  # åªæµ‹è¯•å‰2ä¸ªæ¨¡å‹
            try:
                print(f"   æ­£åœ¨å¼‚æ­¥æµ‹è¯• {model['name']}...")
                response = await llm_manager.asimple_chat(
                    "è¯·åˆ†æä¸€ä¸‹æ‹›è˜è¡Œä¸šçš„å‘å±•è¶‹åŠ¿",
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›è˜å’ŒäººåŠ›èµ„æºåŠ©æ‰‹ã€‚",
                    model['id']
                )
                print(f"   {model['name']} å¼‚æ­¥å›å¤: {response[:150]}...")
            except Exception as e:
                print(f"   âŒ {model['name']} å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")
        
        print("\nâœ… å¼‚æ­¥èŠå¤©æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å¼‚æ­¥LLMç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_conversation():
    """æµ‹è¯•å¤šè½®å¯¹è¯"""
    try:
        print("\n4. æµ‹è¯•å¤šè½®å¯¹è¯:")
        
        models = llm_manager.get_available_models()
        if not models:
            print("   âš ï¸  æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹è¿›è¡Œå¤šè½®å¯¹è¯æµ‹è¯•
        model = models[0]
        print(f"   ä½¿ç”¨æ¨¡å‹: {model['name']}")
        
        # ç¬¬ä¸€è½®å¯¹è¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›è˜é¡¾é—®ã€‚"},
            {"role": "user", "content": "æˆ‘æ˜¯ä¸€ä¸ª3å¹´ç»éªŒçš„å‰ç«¯å·¥ç¨‹å¸ˆï¼Œæƒ³è¦è·³æ§½ï¼Œæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ"}
        ]
        
        response1 = llm_manager.chat(messages, model['id'])
        print(f"   ç¬¬ä¸€è½®å›å¤: {response1[:200]}...")
        
        # ç¬¬äºŒè½®å¯¹è¯
        messages.append({"role": "assistant", "content": response1})
        messages.append({"role": "user", "content": "æˆ‘ä¸»è¦åšReactå¼€å‘ï¼Œæƒ³è¦æå‡æŠ€èƒ½ï¼Œä½ å»ºè®®å­¦ä¹ ä»€ä¹ˆï¼Ÿ"})
        
        response2 = llm_manager.chat(messages, model['id'])
        print(f"   ç¬¬äºŒè½®å›å¤: {response2[:200]}...")
        
        print("\nâœ… å¤šè½®å¯¹è¯æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å¤šè½®å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª LLMç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # åŒæ­¥æµ‹è¯•
    test_llm_manager()
    
    # å¼‚æ­¥æµ‹è¯•
    await test_llm_manager_async()
    
    # å¤šè½®å¯¹è¯æµ‹è¯•
    test_conversation()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main()) 
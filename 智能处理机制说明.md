# 工作经历智能处理机制说明

## 问题背景

在处理简历时，工作经历部分经常出现以下问题：

1. **内容过长**：工作经历通常包含详细的工作描述和成就，容易超过API的token限制
2. **JSON格式错误**：长文本中的特殊字符（换行符、引号等）导致JSON解析失败
3. **信息丢失**：简单的截断处理会导致重要信息丢失

## 解决方案：四层智能处理策略

### 策略1：完整提取（3000 tokens）
- **目标**：尝试一次性提取完整的工作经历信息
- **Token限制**：3000 tokens
- **适用场景**：中等长度的工作经历
- **优势**：保持信息的完整性和结构化

### 策略2：基本信息提取（2000 tokens）
- **目标**：提取关键信息，简化工作描述
- **Token限制**：2000 tokens
- **适用场景**：较长的工作经历
- **特点**：工作描述限制在200字以内

### 策略3：简化格式提取（1000 tokens）
- **目标**：只提取最基本的信息
- **Token限制**：1000 tokens
- **适用场景**：超长工作经历
- **特点**：只保留公司名、职位、时间等核心信息

### 策略4：分块处理（1500 tokens/块）
- **目标**：将超长内容分割成多个块分别处理
- **Token限制**：每个块1500 tokens
- **适用场景**：极长工作经历或多个工作经历
- **特点**：智能识别工作经历边界，分块处理

## 技术实现

### 1. 智能步骤识别
```python
steps = [
    {
        "name": "工作经历",
        "instruction": "...",
        "is_complex": True  # 标记为复杂步骤
    }
]
```

### 2. 工作经历部分识别
- 使用关键词匹配识别工作经历部分
- 支持中英文关键词
- 智能边界识别

### 3. JSON修复机制
- 修复未闭合的字符串引号
- 修复缺少的逗号分隔符
- 多层级的错误恢复

### 4. 分块处理算法
```python
def _identify_work_sections(self, file_content: str) -> List[str]:
    # 识别工作经历部分
    # 智能分割成多个块
    # 返回处理后的块列表
```

## 处理流程

```
开始处理工作经历
    ↓
策略1：尝试完整提取（3000 tokens）
    ↓ (失败)
策略2：基本信息提取（2000 tokens）
    ↓ (失败)
策略3：简化格式提取（1000 tokens）
    ↓ (失败)
策略4：分块处理（1500 tokens/块）
    ↓
返回处理结果
```

## 优势特点

### 1. 渐进式降级
- 从最完整到最简化的处理策略
- 确保在任何情况下都能提取到有用信息
- 避免完全失败的情况

### 2. 智能分块
- 自动识别工作经历的边界
- 避免人工分割的误差
- 保持信息的完整性

### 3. 强大的错误恢复
- 多层级的JSON修复机制
- 自动重试和降级处理
- 详细的错误日志

### 4. 灵活的Token管理
- 根据内容复杂度动态调整token限制
- 避免不必要的token浪费
- 提高处理效率

## 使用示例

### 基本使用
```python
client = KimiClient()
result = client.standardize_resume_by_file(file_id)
# 自动使用智能处理机制
```

### 测试功能
```python
# 运行测试脚本
python test_intelligent_processing.py
```

## 性能优化

### 1. 缓存机制
- 相同内容的处理结果缓存
- 避免重复的API调用

### 2. 并发处理
- 分块处理可以并行执行
- 提高处理速度

### 3. 错误统计
- 记录各种错误类型
- 持续优化处理策略

## 监控和调试

### 1. 详细日志
- 每个策略的执行情况
- JSON修复的详细信息
- 错误原因分析

### 2. 性能指标
- 处理时间统计
- Token使用情况
- 成功率统计

### 3. 质量评估
- 提取信息的完整性
- 数据准确性验证
- 用户反馈收集

## 未来改进

### 1. 机器学习优化
- 基于历史数据优化分块策略
- 智能预测内容长度
- 自动调整处理参数

### 2. 多模型支持
- 支持不同的LLM模型
- 根据模型特性调整策略
- 模型性能对比

### 3. 用户反馈机制
- 收集用户对提取结果的反馈
- 持续优化处理算法
- 个性化处理策略

## 总结

这个智能处理机制通过四层策略和强大的错误恢复能力，能够有效处理各种长度和复杂度的简历工作经历，确保信息的完整性和准确性。同时，通过智能分块和渐进式降级，避免了传统方法中的信息丢失问题。 